---
title: "XG Boost in Mental Health Classification"
subtitle: "Fall 2025 Capstone for Data Science"
author: "Aabiya Mansoor, Abigail Penza Jackson, Corina Rich, Madelyn Champion (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  revealjs:
    smaller: true
    scrollable: true
    theme: moon
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---
```{r}
#| include: false
# Load libraries
library(knitr)
library(tidyverse)
library(xgboost)
library(ggplot2)
library(DataExplorer)
library(dplyr)
library(tidyr)
library(kableExtra)
library(vcd)
library(reshape2)
library(FSelectorRcpp)
library(magrittr) 
library(tidymodels)
library(iml)
library(vip)
library(shapviz)
library(recipes)
library(purrr)
library(rsample)
library(yardstick)
library(gt)
```

## Introduction  {.smaller}

#### eXtreme Gradient Boosting (XGB) algorithm
- A supervised machine learning algorithm
- Modification of Gradient Boosting Framework
- Ensemble of weak decision trees
- L1,L2 regularization
- High performance, speed, scalability
- Applications in healthcare, education, public health, finance, and engineering.

## Introduction {.smaller}

#### XGBoost for mental disorder classification
- Using XGBoost on clinical and survey data
- Classification of individuals into one of four categories: Bipolar I Disorder, Bipolar II Disorder, Major Depressive Disorder, and Normal
- XGBoosts proves to be a suitable algorithm for the problem

## Literature Review {.smaller}

#### eXtreme Gradient Boosting (XGBoost) in healthcare
- XGBoost strengths in real world problems handling class imbalance, heterogeneous data types, or non-linear relationships. [@chen2016xgboost]
- XGBoost combined with DL for breast cancer classification with high reliability. [@liew2021breast]
- XGBoost with biomarker data to improve depression diagnoses in a large Dutch population dataset.[@sharma2020depression]
- XGBoost to multi-modal datasets to predict self-harm in young adults.[@xu2024selfharm]
- A hybrid algorithm (XGBoost-HOA) to classify depression, anxiety, and stress
- Dual XGBoost models applied to distinguish between deficit and non-deficit schizophrenia subtypes using fMRI features.[@zhang2022imbalanced]
- XGBoost compared with linear regression in predicting depression among refugee children.[@saleh2024child] 


## Literature Review {.smaller}

#### XGBoost limitations for imbalance data
- handled by optimization techniques [@zhang2022imbalanced]
- Handled by balancing the class distribution [@sharma2020depression]
- Preprocessing and hyperparameter tuning importance in imbalance data for accurate XGBoost algorithm.

## Literature Review {.smaller}

#### XGBoost in Education
- XGBoost in academic prediction models [@hu2019academic]
- XGBoost out performing traditional logistic regression models in predicting learner performance [@hakkal2024education].

#### XGBoost in Public health
- XGBoost used to predict daily COVID-19 cases in the United States.[@fang2022covid]
- A hybrid model integrating XGBoost, Random Forest, and Antlion Optimization used to predict infectious disease outbreaks.[@sivakumar2023prediction]

## Literature Review {.smaller}

#### XGBoost in financial sector
- XGBoost model’s ability to rank feature importance and prevent overfitting using regularization made it a top performer in credit risk prediction tasks[@li2020xgboost]
- XGBoost to forecast volatility in the U.S. stock market, identifying the Economic Policy Uncertainty Index as a critical predictor.[@fomunyam2023impact]

#### XGBoost In pharmaceutical research
- Application of XGBoost in drug development.[@wiens2025drugdev]

## Literature Review {.smaller}

#### XGBoost in educational diagnostics
- XGBoost enhanced learner performance prediction and knowledge tracing on platforms like ASSIST09 and Algebra08. [@su2023knowledge] and [@hakkal2024education]

#### XGBoost in Sports Analytics
- XGBoost used to predict ultramarathon running speeds based on demographics and environmental factors.[@nikolaidis2023ultramarathon]


## Methods: Extreme Gradient Boosting (XGBoost) {.smaller}

- Ensemble of weak decision trees
- Trained sequentially, each tree corrects previous errors
- Captures non-linear relationships within the 17-variable dataset
- Uses gradient descent to minimize loss
- Includes regularization to prevent overfitting


## Methods: XGBoost Formulas {.smaller}

- Loss term: Measures prediction error
- Regularization term: Penalizes model complexity
- Ensures balance between accuracy and generalization

$$
\text{Obj} = \sum_{i=1}^{n} L(y_i, \hat{y}_i) + \sum_{k=1}^{K} \Omega(f_k)
$$

## Methods: Training the Workflow {.smaller}

- Stage 1: Initial Prediction
- Stage 2: Iterative Boosting (Tree k)
- Stage 3: Final Ensemble Prediction

![XG Boost](XG Boost.png)

## Data Exploration and Visualization {.smaller}

- Sourced from Kaggle (2023 clinical records)
- 120 individuals assessed for mental disorders
-Labels:
   - Normal (25% of total)
   - Bipolar Type I (23% of total)
   - Bipolar Type II (26% of total)
   - Depression (26% of total)
- 17 diagnostic variables
- Mix of clinical, behavioral, demographic factors

## Data Exploration and Visualization {.smaller}

- Examined class distribution
- Identified potential imbalance or outliers
- Explored correlations between features

![Cramers V](Cramers V.png)

## Modeling and Results {.smaller}

#### Data Preprocessing
To facilitate analysis, all categorical variables in the dataset were transformed into numerical or grouped categorical formats in Excel. The transformations were applied as follows:

- Ordinal responses (Sadness, Euphoric, Exhausted) recoded:
  - Seldom=1, Sometimes=2, Usually=3, Most-Often=4
- Binary items (Suicidal Thoughts, Mood Swing):
  - Yes=1, No=0
- 1–10 rating scales (Sexual Activity, Optimism) grouped:
  - 1–3 → Cat 1, 4–6 → Cat 2, 7–9 → Cat 3
  
## Modeling and Results {.smaller}

#### Hyperparameter Optimization

- The dataset was randomly partitioned into an 80/20 training–testing split. 
  - This split resulted in 96 patient surveys used for model training and 24 surveys reserved for independent testing. 
  - The predictive target for all models was the expert clinical diagnosis
  
- An XGBoost model specification was created using the tidymodels framework where all major hyperparameters were set to be tuned including:
  - the number of trees
  - tree depth 
  - learning rate 
  - loss reduction, 
  - minimum node size
  - subsampling rate
  - number of predictors considered at each split 
      
## Modeling and Results {.smaller}

#### Hyperparameter Optimization

- Five-fold stratified cross-validation was used for hyperparameter tuning. 
  - Where the training dataset was partitioned into five equally sized folds
  - For each tuning iteration: 
      - Models were trained on four folds 
      - Validated on the remaining fold
      - Which ensured stable and unbiased performance estimates

![Five Fold Validation](Five Fold Validation.png)[@singh2024crossvalidation_image]

## Modeling and Results {.smaller}

#### Hyperparameter Optimization
- A 30-point hyperparameter grid was generated using a space-filling design.
- Model performance was evaluated using Accuracy and ROC AUC.
- Performance improved with more boosting iterations and depth = 2, peaking at ~625 trees with Accuracy > 0.80 and ROC AUC > 0.90.


![VisulizationTuningResults](VisulizationTuningResults.png)

## Modeling and Results {.smaller}

#### Hyperparameter Optimization
- The select_best() function from the tidymodels framework was used to systematically extract the hyperparameter combination that achieved the highest ROC AUC across all resampled evaluations.

- The resulting optimal configuration included 3 predictors (mtry = 3), 627 boosting iterations, a minimum node size (min_n) of 2, a learning rate of 0.16501, a loss-reduction parameter of 0.0034, and a subsample proportion of 0.5655. 
  - This combination represents the tuning grid entry labeled pre0_mod06_post0, which achieved the best balance between model complexity and predictive performance during cross-validation.

![BestParams](BestParams.png)

## Modeling and Results {.smaller}

#### Model Evaluation & Performance

![Prediction](Prediction.png)

The model's predictions on the held-out test set showed strong agreement with the expert diagnoses. Specifically:

- **Bipolar Type-1:** 4 of 6 cases were correctly predicted (66.7%)  
- **Bipolar Type-2:** 5 of 7 cases were correctly predicted (71.4%)  
- **Depression:** 6 of 7 cases were correctly predicted (85.7%)  
- **Normal:** 4 of 6 cases were correctly predicted (66.7%)  

## Modeling and Results {.smaller}

#### Model Evaluation & Performance

- XGBoost shows **moderate overall predictive performance** with strong discriminative ability.
- **Accuracy:** 0.654 — correctly classifies ~65% of observations.
- **Kappa:** 0.534 — indicates **moderate agreement** beyond chance.
- **Macro ROC AUC:** 0.927 — strong ability to rank observations across all classes.
- Overall: **High AUC but moderate accuracy/kappa**, suggesting strong discrimination but room to improve final classification. 

## Modeling and Results {.smaller}

#### Feature Importance & Interepretability

- SHAP analysis shows both broadly influential and class-specific predictors across the four classes.

- **Most Influential (Across Classes):**
  - **Mood Swing:** strongest overall, especially for Bipolar I & II  
  - **Suicidal Thoughts:** key for Depression and Bipolar II  
  - **Sadness:** major driver for Bipolar I

- **Class-Specific Influences:**
  - **Concentration (Cat 2):** strong indicator for Normal  
  - **Authority & Respect:** most impactful for Bipolar II  
  - **Ignore & Move On** and **Exhausted:** help distinguish the Normal class

- **Conclusion:** Distinct SHAP patterns reveal unique psychological signatures across classes, reflecting a well-calibrated multiclass XGBoost model.

## Modeling and Results {.smaller}

#### Sensitvity & Robustness Analysis

- Sensitivity analysis shows the model is **stable and consistent** across resampling.
  - **Mean Accuracy:** 0.804 — strong predictive performance.  
  - **SD Accuracy:** 0.0821 — moderate variability across samples.  
  - **Mean ROC AUC:** 0.957 — excellent and consistent discriminative ability.  
  - **SD ROC AUC:** 0.0397 — highly stable class separation.
- Overall: **High, stable AUC** and **low–moderate variability**, indicating robust performance with slight accuracy sensitivity to data shifts.

## Modeling and Results {.smaller}

#### Summary of Modeling Results

- On the 24-case test set, the model showed **strong agreement with expert diagnoses** and good generalization.
- It correctly identified most cases in each diagnostic category.

- **Final model strengths:**
  - **High discriminative power:** ROC AUC > 0.90 across CV and sensitivity analyses  
  - **Stable generalization:** low variability across repeats  
  - **Interpretability:** clear clinical insights via SHAP and feature importance  
  - **Accurate predictions:** strong performance across all four diagnostic groups

- **Conclusion:** XGBoost is an effective, interpretable model for mental health classification in a small structured dataset, offering strong predictive value and clinically meaningful insights.

## Conclusion {.smaller}

#### Study Purpose 

- Evaluate effectiveness of XGBoost for predicting mental health outcomes.
- Develop, tune, and validate the model.
- Identify highest‑predictive features and assess ability to capture non‑linear relationships.
- Model showed strong accuracy and generalizability.

## Conclusion {.smaller}

#### Key Findings

- Top predictors: stress indicators, behavioral patterns, wellness factors.
- High feature importance suggests relevance to mental‑health risk.
- Model effectively distinguished between classes.
- Stable performance across tuning configurations with minimal overfitting.

#### Implications

- XGBoost shows potential for early warning mental‑health monitoring.
- Useful for identifying at‑risk individuals based on survey data.
- Self‑reported data may introduce bias—interpret with care.

## Conclusion {.smaller}

#### Future Work
- Use larger and more diverse datasets.
- Incorporate behavioral or time‑based variables.
- Compare with Random Forest, LightGBM, neural networks.
- Test in real‑world settings.
- Address fairness and ethical use of health data.

## Conclusion: Overview {.smaller}

- Overall, XGBoost proved to be an effective and interpretable modeling approach for mental health classification in a small structured dataset, delivering both predictive value and meaningful interpretability essential for clinical research. Machine-learning approaches may provide timely insights that complement traditional assessment methods and support early identification of individuals who may benefit from targeted intervention. With further refinement and validation, predictive modeling has the potential to become an increasingly valuable component of mental-health research and practice.


## References

---
title: "Using XGBoost in Mental Disorder Classification"
subtitle: "Fall 2025"
author: "Aabiya Mansoor, Abigail Penza Jackson, Corina Rich, Madelyn Champion (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    theme: lux
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

## Introduction

eXtreme Gradient Boosting (XGBoost) is a state-of-the-art, supervised machine learning algorithm renowned for its high performance, speed, and scalability [@zhang2022imbalanced]. Developed as an optimized implementation of the gradient boosting framework, XGBoost builds a powerful predictive model by sequentially creating an ensemble of weak decision trees, where each new tree is trained to correct the residual errors of the previous ones [@zhang2022imbalanced]. Its importance in the data science community stems from several key architectural advantages, including a sparsity-aware algorithm for handling missing or zero-value data, parallel processing capabilities for faster computation, and built-in L1 and L2 regularization to prevent model overfitting. The algorithm's versatility and effectiveness have been demonstrated across a wide range of domains, from achieving high accuracy in financial credit scoring [@li2020xgboost] to enhancing learner performance prediction in education [@hakkal2024education].

This research applies the power of XGBoost to address the significant and growing challenge of mental disorder classification. The early and accurate identification of mental health conditions is crucial for effective intervention, yet traditional diagnostic methods can be subjective, resource intensive, and inaccessible to many. Structured clinical and survey data provide an opportunity to apply computational methods to find objective patterns that may aid in this diagnostic process. The objective of this study is to develop and evaluate a robust XGBoost model for the multi-class classification of individuals into one of four categories: Bipolar I Disorder, Bipolar II Disorder, Major Depressive Disorder, and Normal [@chahar2024multiclass].

The selection of XGBoost for this complex classification problem, containing information of 120 patients associated with 17 symptoms, is particularly strategic. The dataset consists of tabular survey responses, where the relationships between different answers (features) and the final diagnosis are complex and non-linear. XGBoost excels at identifying these intricate patterns in structured data. Its ability to perform feature importance ranking will also help in understanding which survey questions are most predictive of a specific mental health condition. Furthermore, XGBoost's built-in regularization is crucial for preventing the model from overfitting to the training data, ensuring it can generalize accurately to new, unseen patient cases. These features make XGBoost an exceptionally suitable framework for the nuanced task of classifying mental health disorders based on categorical survey data.

###### Literature Review

eXtreme Gradient Boosting (XGBoost) has solidified its reputation as a high-performance, scalable, and adaptable machine learning algorithm widely applied across disciplines such as healthcare, education, public health, finance, and engineering. Initially introduced by Chen and Guestrin [@chen2016xgboost], XGBoost builds its predictive strength by constructing an ensemble of weak learners typically decision trees—where each subsequent model corrects the errors of its predecessor. Its unique architectural advantages, including a sparsity-aware algorithm, weighted quantile sketch, parallelized learning, and built-in L1/L2 regularization, contribute to its speed and accuracy, particularly when handling missing values and high dimensional data. These strengths have made XGBoost especially effective in real world data challenges that involve class imbalance, heterogeneous data types, or non-linear relationships.

In the healthcare domain, XGBoost has been instrumental in diagnosing and predicting diseases with enhanced accuracy and speed. For example, Liew et al. [@liew2021breast] used a hybrid model combining deep learning and XGBoost for breast cancer classification based on histopathological images, demonstrating its ability to detect and differentiate cancer types with high reliability. Similarly, Sharma and Verbeke [@sharma2020depression] leveraged XGBoost with biomarker data to improve depression diagnoses in a large Dutch population dataset. Their findings highlighted the importance of resampling techniques to counter data imbalance, as oversampled models achieved precision and recall scores above 0.90. Xu et al. [@xu2024selfharm] applied XGBoost to multi-modal datasets to predict self-harm in young adults, achieving a balanced accuracy of 0.800, and identifying non-linear predictors such as suicidal ideation and NTRK2 gene variants. Chahar et al. [@chahar2024multiclass] further extended the model’s mental health applications by using a hybrid XGBoost–Hippopotamus Optimization Algorithm (XGBoost-HOA) to classify depression, anxiety, and stress, with SMOTE for resampling, achieving accuracies exceeding 77% across all categories.

In studies involving imbalanced data, XGBoost’s limitations are addressed through optimization techniques. Zhang et al. [@zhang2022imbalanced] examined its application in cloud based sensor networks and demonstrated how performance improved significantly when Bayesian optimization and mixed sampling techniques were used. Their approach, evaluated using G-mean and AUC metrics, outperformed baseline XGBoost models. Likewise, Sharma and Verbeke [@sharma2020depression] found that without balancing the class distribution, XGBoost performed poorly, but once sampling strategies were applied, performance metrics improved dramatically. These findings illustrate the necessity of preprocessing and hyperparameter tuning for extracting XGBoost’s full potential in imbalanced datasets.

Beyond healthcare, XGBoost has been applied in academic prediction models. Hu and Song [@hu2019academic] used the algorithm to forecast semester grades of college students based on previous academic performance. Although the accuracy reached only 55%, the study praised XGBoost for its low resource consumption and computational efficiency, making it a suitable choice for educational data mining. Hakkal and Lahcen [@hakkal2024education] further demonstrated the model’s superiority over traditional logistic regression models in predicting learner performance, reporting an increase in AUC scores from 0.690 to 0.709. Su et al. [@su2023knowledge] showed even more compelling results in online education platforms, where XGBoost achieved an AUC score of 0.9855 while remaining more time efficient in training than deep learning models.

In the realm of public health and epidemiology, Fang et al. [@fang2022covid] used XGBoost to predict daily COVID-19 cases in the United States. The study found that XGBoost significantly outperformed traditional ARIMA models in time series forecasting, although the authors noted limitations when the model was applied outside the initial geographic context or when data reporting declined. Similarly, a hybrid model integrating XGBoost, Random Forest, and Antlion Optimization was deployed in India to predict infectious disease outbreaks. With over 21,000 samples, the model achieved over 96% accuracy, but the results were constrained by regional data limitations [@sivakumar2023prediction].

Applications in the financial sector also highlight XGBoost’s utility. Li et al. [@li2020xgboost] demonstrated its superiority over logistic regression in predicting loan defaults using the Lending Club dataset. The model’s ability to rank feature importance and prevent overfitting using regularization made it a top performer in credit risk prediction tasks. Similarly, Fomunyam [@fomunyam2023impact] applied XGBoost to forecast volatility in the U.S. stock market, identifying the Economic Policy Uncertainty Index as a critical predictor. While the model achieved modest accuracy (55.62%) and a low MCC (0.2793), it proved useful in highlighting economic indicators driving market fear.

In pharmaceutical research, Wiens et al. [@wiens2025drugdev] presented a comprehensive tutorial and use case for applying XGBoost in drug development. The study used the algorithm to predict disease risk progression, showcasing its effectiveness in clinical decision making contexts. The model's capacity to handle missing data, rank feature importance, and outperform traditional models in clinical trial data made it a valuable tool in the biomedical pipeline.

Further applications of XGBoost include educational diagnostics and sports analytics. For instance, Su et al. [@su2023knowledge] and Hakkal and Lahcen [@hakkal2024education] showed how XGBoost enhanced learner performance prediction and knowledge tracing on platforms like ASSIST09 and Algebra08. Nikolaidis et al. [@nikolaidis2023ultramarathon] used XGBoost to predict ultramarathon running speeds based on demographics and environmental factors, finding that country of origin and road surface type were strong predictors. In construction, Ren et al. [@ren2023strength] utilized XGBoost to predict the compressive strength of ultra-high-performance concrete, attaining 95.6% accuracy. However, initial overfitting required hyperparameter optimization, and the lack of data normalization was cited as a limitation.

XGBoost has also been instrumental in mental health classification studies using small or imbalanced datasets. Zhu, Shen, and Zhang [@zhang2022imbalanced] applied dual XGBoost models to distinguish between deficit and non-deficit schizophrenia subtypes using fMRI features, reaching an average accuracy of 73.89%. Saleh et al. [@saleh2024child] compared XGBoost with linear regression in predicting depression among refugee children, finding that XGBoost offered a richer, non-linear understanding of contributing factors such as sleep quality and stress. Lastly, a recent study applied XGBoost to classify mental health disorders, including Bipolar I, Bipolar II, Major Depressive Disorder, and Normal, using categorical symptom data. The model’s ability to rank symptom importance and identify complex patterns made it ideal for psychiatric diagnostics using structured survey inputs.

Altogether, these findings reinforce XGBoost’s versatility and robustness in handling high dimensional, imbalanced, and heterogeneous data. While performance can be compromised without proper preprocessing or tuning, the algorithm consistently outperforms traditional statistical models when paired with sampling strategies, optimization algorithms, and domain specific enhancements. Its strength lies in its balance of computational efficiency, predictive power, and interpretability, making it a benchmark tool in the expanding landscape of applied machine learning.



## Methods

### eXtreme Gradient Boosting (XGBoost) Model

The eXtreme Gradient Boosting (XGBoost) algorithm is a powerful ensemble learning method based on gradient-boosted decision trees. It builds an additive model in a forward stage-wise manner, where each new tree is trained to predict the residual errors of the ensemble built thus far. This iterative process allows the model to capture complex, non-linear relationships and interactions between features—in this case, across the 17 variables in the dataset.

XGBoost optimizes a regularized objective function that balances model accuracy with complexity, thereby reducing the risk of overfitting. The general form of the objective function is:

$$
\text{Obj} = \sum_{i=1}^{n} L(y_i, \hat{y}_i) + \sum_{k=1}^{K} \Omega(f_k)
$$

Where:

- The **loss function** measures the difference between the predicted and actual values:

$$
L(y_i, \hat{y}_i) = (y_i - \hat{y}_i)^2
$$

- The **regularization term** penalizes the complexity of each tree:

$$
\Omega(f_k) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^{T} w_j^2
$$

Here:

$$
T = \text{number of leaves in the tree}
$$

$$
w_j = \text{score (weight) on leaf } j
$$

$$
\gamma = \text{penalty for adding a new leaf}
$$

$$
\lambda = \text{L2 regularization term on leaf weights}
$$

Gradient descent is used to minimize this objective function by updating model parameters iteratively. The regularization components play a key role in controlling tree complexity, helping prevent overfitting—especially in high-dimensional or noisy datasets.

Hyperparameters such as the learning rate, maximum tree depth, regularization strength, and subsampling ratio are typically tuned via cross-validation to maximize model performance. Due to its efficiency and predictive power, XGBoost is widely used in structured data problems [@zhang2022imbalanced].


## Analysis and Results

### Data Exploration and Visualization

The dataset used for this analysis was sourced from Kaggle (https://www.kaggle.com/datasets/cid007/mental-disorder-classification) and originates from a private psychologist's office, capturing patient data collected throughout 2023. It includes information on 120 individuals who were assessed for mental health conditions. Each patient was diagnosed with one of four possible mental health statuses: normal, bipolar type I, bipolar type II, or depression. The dataset contains 17 variables that were used to inform the diagnostic process, potentially covering a range of clinical, behavioral, and demographic factors. These variables form the basis for classification and analysis of the mental health conditions represented. Table 1 provides a detailed overview of each variable included in the dataset, offering insight into the features considered during the diagnostic evaluation.

```{r}
# Create Table 1: Variable Names and Descriptions

# Define variables and descriptions
variables <- c(
  "Patient Number",
  "Sadness",
  "Euphoric",
  "Exhausted",
  "Sleep Disorder",
  "Mood Swing",
  "Suicidal Thoughts",
  "Anorexia",
  "Authority Respect",
  "Try-Explanation",
  "Aggressive Response",
  "Ignore & Move-On",
  "Nervous Breakdown",
  "Admit Mistakes",
  "Overthinking",
  "Sexual Activity",
  "Concentration",
  "Optimism",
  "Expert Diagnose"
)

descriptions <- c(
  "Unique identifier for each patient.",
  "How often the patient experiences sadness determined by 4 groupings (Seldom, Sometimes, Usually, Most-Often).",
  "How often the patient experiences a euphoric mood determined by 4 groupings (Seldom, Sometimes, Usually, Most-Often).",
  "How often the patient experiences feelings of physical or mental fatigue determined by 4 groupings (Seldom, Sometimes, Usually, Most-Often).",
  "How often the patient experiences presence and severity of sleep-related disturbances determined by 4 groupings (Seldom, Sometimes, Usually, Most-Often).",
  "If the patient emotional mood swings (Yes/No) response.",
  "If the patient has suicidal ideation (Yes/No) response.",
  "If the patient has indicators of disordered eating or significant weight loss (Yes/No) response.",
  "If the patient respects authority and rules (Yes/No) response.",
  "If the patient tries to explain their behavior or symptoms (Yes/No) response.",
  "If the patient has a tendency to respond to situations with aggression (Yes/No) response.",
  "If the patient has a tendency to dismiss issues and move forward without resolution (Yes/No) response.",
  "If the patient has incidences of emotional or psychological breakdowns (Yes/No) response.",
  "If the patient has a willingness to admit personal faults or mistakes (Yes/No) response.",
  "If the patient has a tendency to ruminate or overanalyze situations (Yes/No) response.",
  "Level of sexual interest or activity, ranked from 1 to 10.",
  "Ability to focus and maintain attention, ranked from 1 to 10.",
  "General outlook on life and future events ranked from 1 to 10.",
  "Final clinical diagnosis assigned by the expert (Normal, Bipolar I, Bipolar II, Depression)."
)

# Create data frame
table1 <- data.frame(
  Variable = variables,
  Description = descriptions,
  stringsAsFactors = FALSE
)

# Print Table 1
library(knitr)

kable(table1, format = "html", table.attr = "style='width:100%;'")  # for HTML output


```

##### Initial Data Exploration

```{r}
#| include: false

# Set CRAN mirror explicitly
options(repos = c(CRAN = "https://cloud.r-project.org"))

# Install missing packages only
required_packages <- c("xgboost", "DataExplorer", "tidyverse", "ggplot2", "dplyr")

for (pkg in required_packages) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg, dependencies = TRUE)
  }
}

# Load libraries
library(tidyverse)
library(xgboost)
library(ggplot2)
library(DataExplorer)
library(dplyr)

```

```{r}
data <- read.csv("Dataset-Mental-Disorders.csv", stringsAsFactors = FALSE)
# Drop Patient Number column 
data <- data %>% select(-`Patient.Number`)
```


```{r}
library(dplyr)
library(tidyr)
library(ggplot2)

# Define variables to combine
included_vars <- c("Sexual.Activity", "Concentration", "Optimism")

# Pivot data to longer format for these variables
long_data <- data %>%
  select(all_of(included_vars)) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Response")

# Plot grouped bar chart
ggplot(long_data, aes(x = Response, fill = Variable)) +
  geom_bar(position = "dodge") +  # dodge = side-by-side bars
  labs(
    title = "Response Distribution for Sexual Activity, Concentration, and Optimism",
    x = "Response",
    y = "Count",
    fill = "Variable"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

```

The data appears to be noramlly distributed across the different variables and features.

```{r}
#| echo: false
library(dplyr)
library(tidyr)
library(knitr)
library(kableExtra)

# List of variables to exclude
excluded_vars <- c("Sexual.Activity", "Concentration", "Optimism")

# Create distribution table, excluding the listed variables
distribution_table <- data %>%
  select(-all_of(excluded_vars)) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value") %>%
  group_by(Variable, Value) %>%
  summarise(Count = n(), .groups = "drop") %>%
  arrange(Variable, desc(Count)) %>%
  group_by(Variable) %>%
  summarise(Distribution = paste(Value, Count, sep = ": ", collapse = ", "), .groups = "drop")

# Format nicely
distribution_table %>%
  kbl(
    caption = "Category Distributions per Categorical Variable",
    align = c("l", "l"),
    col.names = c("Variable", "Category Distribution"),
    escape = FALSE  # Corrected from earlier error
  ) %>%
  kable_styling(
    full_width = FALSE,
    bootstrap_options = c("striped", "hover", "condensed", "responsive"),
    position = "center"
  ) %>%
  column_spec(2, width = "40em")
```




```{r}
#| include: false
# Separate target variable
target <- data$Expert.Diagnose

# Drop target from the features dataset
features <- data %>% select(-"Expert.Diagnose")

# Install needed package
install.packages("vcd", dependencies = TRUE)
library(vcd)

# Compute Cramer's V for each feature against the target
cramers_v <- sapply(names(features), function(col) {
  tbl <- table(features[[col]], target)
  assocstats(tbl)$cramer
})

# Display results
cramers_v <- sort(cramers_v, decreasing = TRUE)
cramers_v
```

```{r}
# Convert to data frame for plotting
library(ggplot2)

cramers_df <- data.frame(
  Variable = names(cramers_v),
  CramersV = cramers_v
)

# Drop NA values
cramers_df <- cramers_df[!is.na(cramers_df$CramersV), ]

# Plot
ggplot(cramers_df, aes(x = reorder(Variable, CramersV), y = CramersV)) +
  geom_col(fill = "#2c7fb8") +
  coord_flip() +
  labs(title = "Cramer's V Association with Mental Health Diagnosis",
       x = "Feature",
       y = "Cramer's V") +
  theme_minimal()

```
Using Cramer's V we are able to see that mood swings and suicidal thoughs have the highest correlation to the mental health diagnosis. 

##### Data Transformation & Cleaning 
To facilitate analysis, all categorical variables in the dataset were transformed into numerical or grouped categorical formats. The transformations were applied as follows:

* **Ordinal Frequency Responses** (e.g., *Sadness*, *Euphoric*, *Exhausted*):

  * *Seldom* → 1
  * *Sometimes* → 2
  * *Usually* → 3
  * *Most-Often* → 4

* **Binary Responses** (e.g., *Suicidal Thoughts*, *Mood Swing*):

  * *Yes* → 1
  * *No* → 0

* **Scaled Ratings (1–10)** (e.g., *Sexual Activity*, *Optimism*):
  These were grouped into three categorical levels to simplify analysis:

  * Ratings **1–3** → *Category 1*
  * Ratings **4–6** → *Category 2*
  * Ratings **7–9** → *Category 3*

These transformations standardized the dataset, allowing for easier correlation analysis, visualizations, and model input compatibility.


### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References


---
title: "Using XGBoost in Mental Disorder Classification"
subtitle: "Fall 2025"
author: "Aabiya Mansoor, Abigail Penza Jackson, Corina Rich, Madelyn Champion (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    theme: lux
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

## Introduction

eXtreme Gradient Boosting (XGBoost) is a state-of-the-art, supervised machine learning algorithm renowned for its high performance, speed, and scalability [@zhang2022imbalanced]. Developed as an optimized implementation of the gradient boosting framework, XGBoost builds a powerful predictive model by sequentially creating an ensemble of weak decision trees, where each new tree is trained to correct the residual errors of the previous ones [@zhang2022imbalanced]. Its importance in the data science community stems from several key architectural advantages, including a sparsity-aware algorithm for handling missing or zero-value data, parallel processing capabilities for faster computation, and built-in L1 and L2 regularization to prevent model overfitting. The algorithm's versatility and effectiveness have been demonstrated across a wide range of domains, from achieving high accuracy in financial credit scoring [@li2020xgboost] to enhancing learner performance prediction in education [@hakkal2024education].

This research applies the power of XGBoost to address the significant and growing challenge of mental disorder classification. The early and accurate identification of mental health conditions is crucial for effective intervention, yet traditional diagnostic methods can be subjective, resource intensive, and inaccessible to many. Structured clinical and survey data provide an opportunity to apply computational methods to find objective patterns that may aid in this diagnostic process. The objective of this study is to develop and evaluate a robust XGBoost model for the multi-class classification of individuals into one of four categories: Bipolar I Disorder, Bipolar II Disorder, Major Depressive Disorder, and Normal [@chahar2024multiclass].

The selection of XGBoost for this complex classification problem, containing information of 120 patients associated with 17 symptoms, is particularly strategic. The dataset consists of tabular survey responses, where the relationships between different answers (features) and the final diagnosis are complex and non-linear. XGBoost excels at identifying these intricate patterns in structured data. Its ability to perform feature importance ranking will also help in understanding which survey questions are most predictive of a specific mental health condition. Furthermore, XGBoost's built-in regularization is crucial for preventing the model from overfitting to the training data, ensuring it can generalize accurately to new, unseen patient cases. These features make XGBoost an exceptionally suitable framework for the nuanced task of classifying mental health disorders based on categorical survey data.

###### Literature Review

eXtreme Gradient Boosting (XGBoost) has solidified its reputation as a high-performance, scalable, and adaptable machine learning algorithm widely applied across disciplines such as healthcare, education, public health, finance, and engineering. Initially introduced by Chen and Guestrin [@chen2016xgboost], XGBoost builds its predictive strength by constructing an ensemble of weak learners typically decision trees—where each subsequent model corrects the errors of its predecessor. Its unique architectural advantages, including a sparsity-aware algorithm, weighted quantile sketch, parallelized learning, and built-in L1/L2 regularization, contribute to its speed and accuracy, particularly when handling missing values and high dimensional data. These strengths have made XGBoost especially effective in real world data challenges that involve class imbalance, heterogeneous data types, or non-linear relationships.

In the healthcare domain, XGBoost has been instrumental in diagnosing and predicting diseases with enhanced accuracy and speed. For example, Liew et al. [@liew2021breast] used a hybrid model combining deep learning and XGBoost for breast cancer classification based on histopathological images, demonstrating its ability to detect and differentiate cancer types with high reliability. Similarly, Sharma and Verbeke [@sharma2020depression] leveraged XGBoost with biomarker data to improve depression diagnoses in a large Dutch population dataset. Their findings highlighted the importance of resampling techniques to counter data imbalance, as oversampled models achieved precision and recall scores above 0.90. Xu et al. [@xu2024selfharm] applied XGBoost to multi-modal datasets to predict self-harm in young adults, achieving a balanced accuracy of 0.800, and identifying non-linear predictors such as suicidal ideation and NTRK2 gene variants. Chahar et al. [@chahar2024multiclass] further extended the model’s mental health applications by using a hybrid XGBoost–Hippopotamus Optimization Algorithm (XGBoost-HOA) to classify depression, anxiety, and stress, with SMOTE for resampling, achieving accuracies exceeding 77% across all categories.

In studies involving imbalanced data, XGBoost’s limitations are addressed through optimization techniques. Zhang et al. [@zhang2022imbalanced] examined its application in cloud based sensor networks and demonstrated how performance improved significantly when Bayesian optimization and mixed sampling techniques were used. Their approach, evaluated using G-mean and AUC metrics, outperformed baseline XGBoost models. Likewise, Sharma and Verbeke [@sharma2020depression] found that without balancing the class distribution, XGBoost performed poorly, but once sampling strategies were applied, performance metrics improved dramatically. These findings illustrate the necessity of preprocessing and hyperparameter tuning for extracting XGBoost’s full potential in imbalanced datasets.

Beyond healthcare, XGBoost has been applied in academic prediction models. Hu and Song [@hu2019academic] used the algorithm to forecast semester grades of college students based on previous academic performance. Although the accuracy reached only 55%, the study praised XGBoost for its low resource consumption and computational efficiency, making it a suitable choice for educational data mining. Hakkal and Lahcen [@hakkal2024education] further demonstrated the model’s superiority over traditional logistic regression models in predicting learner performance, reporting an increase in AUC scores from 0.690 to 0.709. Su et al. [@su2023knowledge] showed even more compelling results in online education platforms, where XGBoost achieved an AUC score of 0.9855 while remaining more time efficient in training than deep learning models.

In the realm of public health and epidemiology, Fang et al. [@fang2022covid] used XGBoost to predict daily COVID-19 cases in the United States. The study found that XGBoost significantly outperformed traditional ARIMA models in time series forecasting, although the authors noted limitations when the model was applied outside the initial geographic context or when data reporting declined. Similarly, a hybrid model integrating XGBoost, Random Forest, and Antlion Optimization was deployed in India to predict infectious disease outbreaks. With over 21,000 samples, the model achieved over 96% accuracy, but the results were constrained by regional data limitations [@sivakumar2023prediction].

Applications in the financial sector also highlight XGBoost’s utility. Li et al. [@li2020xgboost] demonstrated its superiority over logistic regression in predicting loan defaults using the Lending Club dataset. The model’s ability to rank feature importance and prevent overfitting using regularization made it a top performer in credit risk prediction tasks. Similarly, Fomunyam [@fomunyam2023impact] applied XGBoost to forecast volatility in the U.S. stock market, identifying the Economic Policy Uncertainty Index as a critical predictor. While the model achieved modest accuracy (55.62%) and a low MCC (0.2793), it proved useful in highlighting economic indicators driving market fear.

In pharmaceutical research, Wiens et al. [@wiens2025drugdev] presented a comprehensive tutorial and use case for applying XGBoost in drug development. The study used the algorithm to predict disease risk progression, showcasing its effectiveness in clinical decision making contexts. The model's capacity to handle missing data, rank feature importance, and outperform traditional models in clinical trial data made it a valuable tool in the biomedical pipeline.

Further applications of XGBoost include educational diagnostics and sports analytics. For instance, Su et al. [@su2023knowledge] and Hakkal and Lahcen [@hakkal2024education] showed how XGBoost enhanced learner performance prediction and knowledge tracing on platforms like ASSIST09 and Algebra08. Nikolaidis et al. [@nikolaidis2023ultramarathon] used XGBoost to predict ultramarathon running speeds based on demographics and environmental factors, finding that country of origin and road surface type were strong predictors. In construction, Ren et al. [@ren2023strength] utilized XGBoost to predict the compressive strength of ultra-high-performance concrete, attaining 95.6% accuracy. However, initial overfitting required hyperparameter optimization, and the lack of data normalization was cited as a limitation.

XGBoost has also been instrumental in mental health classification studies using small or imbalanced datasets. Zhu, Shen, and Zhang [@zhang2022imbalanced] applied dual XGBoost models to distinguish between deficit and non-deficit schizophrenia subtypes using fMRI features, reaching an average accuracy of 73.89%. Saleh et al. [@saleh2024child] compared XGBoost with linear regression in predicting depression among refugee children, finding that XGBoost offered a richer, non-linear understanding of contributing factors such as sleep quality and stress. Lastly, a recent study applied XGBoost to classify mental health disorders, including Bipolar I, Bipolar II, Major Depressive Disorder, and Normal, using categorical symptom data. The model’s ability to rank symptom importance and identify complex patterns made it ideal for psychiatric diagnostics using structured survey inputs.

Altogether, these findings reinforce XGBoost’s versatility and robustness in handling high dimensional, imbalanced, and heterogeneous data. While performance can be compromised without proper preprocessing or tuning, the algorithm consistently outperforms traditional statistical models when paired with sampling strategies, optimization algorithms, and domain specific enhancements. Its strength lies in its balance of computational efficiency, predictive power, and interpretability, making it a benchmark tool in the expanding landscape of applied machine learning.



## Methods

-   Detail the models or algorithms used.

-   Justify your choices based on the problem and data.

*The common non-parametric regression model is*
$Y_i = m(X_i) + \varepsilon_i$*, where* $Y_i$ *can be defined as the sum
of the regression function value* $m(x)$ *for* $X_i$*. Here* $m(x)$ *is
unknown and* $\varepsilon_i$ *some errors. With the help of this
definition, we can create the estimation for local averaging i.e.*
$m(x)$ *can be estimated with the product of* $Y_i$ *average and* $X_i$
*is near to* $x$*. In other words, this means that we are discovering
the line through the data points with the help of surrounding data
points. The estimation formula is printed below [@R-base]:*

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$$W_n(x)$ *is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if* $X_i$ *is far from* $x$*.*

*Another equation:*

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data Exploration and Visualization

The dataset used for this analysis was sourced from Kaggle and originates from a private psychologist's office, capturing patient data collected throughout 2023. It includes information on 120 individuals who were assessed for mental health conditions. Each patient was diagnosed with one of four possible mental health statuses: normal, bipolar type I, bipolar type II, or depression. The dataset contains 17 variables that were used to inform the diagnostic process, potentially covering a range of clinical, behavioral, and demographic factors. These variables form the basis for classification and analysis of the mental health conditions represented. Table 1 provides a detailed overview of each variable included in the dataset, offering insight into the features considered during the diagnostic evaluation.

```{r}
# Create Table 1: Variable Names and Descriptions

# Define variables and descriptions
variables <- c(
  "Patient Number",
  "Sadness",
  "Euphoric",
  "Exhausted",
  "Sleep Disorder",
  "Mood Swing",
  "Suicidal Thoughts",
  "Anorexia",
  "Authority Respect",
  "Try-Explanation",
  "Aggressive Response",
  "Ignore & Move-On",
  "Nervous Breakdown",
  "Admit Mistakes",
  "Overthinking",
  "Sexual Activity",
  "Concentration",
  "Optimism",
  "Expert Diagnose"
)

descriptions <- c(
  "Unique identifier for each patient.",
  "How often the patient experiences sadness determined by 4 groupings (Seldom, Sometimes, Usually, Most-Often).",
  "How often the patient experiences a euphoric mood determined by 4 groupings (Seldom, Sometimes, Usually, Most-Often).",
  "How often the patient experiences feelings of physical or mental fatigue determined by 4 groupings (Seldom, Sometimes, Usually, Most-Often).",
  "How often the patient experiences presence and severity of sleep-related disturbances determined by 4 groupings (Seldom, Sometimes, Usually, Most-Often).",
  "If the patient emotional mood swings (Yes/No) response.",
  "If the patient has suicidal ideation (Yes/No) response.",
  "If the patient has indicators of disordered eating or significant weight loss (Yes/No) response.",
  "If the patient respects authority and rules (Yes/No) response.",
  "If the patient tries to explain their behavior or symptoms (Yes/No) response.",
  "If the patient has a tendency to respond to situations with aggression (Yes/No) response.",
  "If the patient has a tendency to dismiss issues and move forward without resolution (Yes/No) response.",
  "If the patient has incidences of emotional or psychological breakdowns (Yes/No) response.",
  "If the patient has a willingness to admit personal faults or mistakes (Yes/No) response.",
  "If the patient has a tendency to ruminate or overanalyze situations (Yes/No) response.",
  "Level of sexual interest or activity, ranked from 1 to 10.",
  "Ability to focus and maintain attention, ranked from 1 to 10.",
  "General outlook on life and future events ranked from 1 to 10.",
  "Final clinical diagnosis assigned by the expert (Normal, Bipolar I, Bipolar II, Depression)."
)

# Create data frame
table1 <- data.frame(
  Variable = variables,
  Description = descriptions,
  stringsAsFactors = FALSE
)

# Print Table 1
library(knitr)

kable(table1, format = "html", table.attr = "style='width:100%;'")  # for HTML output


```


```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{python}
import pandas as pd
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References


---
title: "Using XGBoost in Mental Disorder Classification"
subtitle: "Fall 2025"
author: "Aabiya Mansoor, Abigail Penza Jackson, Corina Rich, Madelyn Champion (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    theme: lux
    code-fold: true
toc: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

## Introduction

eXtreme Gradient Boosting (XGBoost) is a state-of-the-art, supervised machine learning algorithm renowned for its high performance, speed, and scalability [@zhang2022imbalanced]. Developed as an optimized implementation of the gradient boosting framework, XGBoost builds a powerful predictive model by sequentially creating an ensemble of weak decision trees, where each new tree is trained to correct the residual errors of the previous ones [@zhang2022imbalanced]. Its importance in the data science community stems from several key architectural advantages, including a sparsity-aware algorithm for handling missing or zero-value data, parallel processing capabilities for faster computation, and built-in L1 and L2 regularization to prevent model overfitting. The algorithm's versatility and effectiveness have been demonstrated across a wide range of domains, from achieving high accuracy in financial credit scoring [@li2020xgboost] to enhancing learner performance prediction in education [@hakkal2024education].

This research applies the power of XGBoost to address the significant and growing challenge of mental disorder classification. The early and accurate identification of mental health conditions is crucial for effective intervention, yet traditional diagnostic methods can be subjective, resource intensive, and inaccessible to many. Structured clinical and survey data provide an opportunity to apply computational methods to find objective patterns that may aid in this diagnostic process. The objective of this study is to develop and evaluate a robust XGBoost model for the multi-class classification of individuals into one of four categories: Bipolar I Disorder, Bipolar II Disorder, Major Depressive Disorder, and Normal [@chahar2024multiclass].

The selection of XGBoost for this complex classification problem, containing information of 120 patients associated with 17 symptoms, is particularly strategic. The dataset consists of tabular survey responses, where the relationships between different answers (features) and the final diagnosis are complex and non-linear. XGBoost excels at identifying these intricate patterns in structured data. Its ability to perform feature importance ranking will also help in understanding which survey questions are most predictive of a specific mental health condition. Furthermore, XGBoost's built-in regularization is crucial for preventing the model from overfitting to the training data, ensuring it can generalize accurately to new, unseen patient cases. These features make XGBoost an exceptionally suitable framework for the nuanced task of classifying mental health disorders based on categorical survey data.

###### Literature Review

eXtreme Gradient Boosting (XGBoost) has solidified its reputation as a high-performance, scalable, and adaptable machine learning algorithm widely applied across disciplines such as healthcare, education, public health, finance, and engineering. Initially introduced by Chen and Guestrin [@chen2016xgboost], XGBoost builds its predictive strength by constructing an ensemble of weak learners typically decision trees—where each subsequent model corrects the errors of its predecessor. Its unique architectural advantages, including a sparsity-aware algorithm, weighted quantile sketch, parallelized learning, and built-in L1/L2 regularization, contribute to its speed and accuracy, particularly when handling missing values and high dimensional data. These strengths have made XGBoost especially effective in real world data challenges that involve class imbalance, heterogeneous data types, or non-linear relationships.

In the healthcare domain, XGBoost has been instrumental in diagnosing and predicting diseases with enhanced accuracy and speed. For example, Liew et al. [@liew2021breast] used a hybrid model combining deep learning and XGBoost for breast cancer classification based on histopathological images, demonstrating its ability to detect and differentiate cancer types with high reliability. Similarly, Sharma and Verbeke [@sharma2020depression] leveraged XGBoost with biomarker data to improve depression diagnoses in a large Dutch population dataset. Their findings highlighted the importance of resampling techniques to counter data imbalance, as oversampled models achieved precision and recall scores above 0.90. Xu et al. [@xu2024selfharm] applied XGBoost to multi-modal datasets to predict self-harm in young adults, achieving a balanced accuracy of 0.800, and identifying non-linear predictors such as suicidal ideation and NTRK2 gene variants. Chahar et al. [@chahar2024multiclass] further extended the model’s mental health applications by using a hybrid XGBoost–Hippopotamus Optimization Algorithm (XGBoost-HOA) to classify depression, anxiety, and stress, with SMOTE for resampling, achieving accuracies exceeding 77% across all categories.

In studies involving imbalanced data, XGBoost’s limitations are addressed through optimization techniques. Zhang et al. [@zhang2022imbalanced] examined its application in cloud based sensor networks and demonstrated how performance improved significantly when Bayesian optimization and mixed sampling techniques were used. Their approach, evaluated using G-mean and AUC metrics, outperformed baseline XGBoost models. Likewise, Sharma and Verbeke [@sharma2020depression] found that without balancing the class distribution, XGBoost performed poorly, but once sampling strategies were applied, performance metrics improved dramatically. These findings illustrate the necessity of preprocessing and hyperparameter tuning for extracting XGBoost’s full potential in imbalanced datasets.

Beyond healthcare, XGBoost has been applied in academic prediction models. Hu and Song [@hu2019academic] used the algorithm to forecast semester grades of college students based on previous academic performance. Although the accuracy reached only 55%, the study praised XGBoost for its low resource consumption and computational efficiency, making it a suitable choice for educational data mining. Hakkal and Lahcen [@hakkal2024education] further demonstrated the model’s superiority over traditional logistic regression models in predicting learner performance, reporting an increase in AUC scores from 0.690 to 0.709. Su et al. [@su2023knowledge] showed even more compelling results in online education platforms, where XGBoost achieved an AUC score of 0.9855 while remaining more time efficient in training than deep learning models.

In the realm of public health and epidemiology, Fang et al. [@fang2022covid] used XGBoost to predict daily COVID-19 cases in the United States. The study found that XGBoost significantly outperformed traditional ARIMA models in time series forecasting, although the authors noted limitations when the model was applied outside the initial geographic context or when data reporting declined. Similarly, a hybrid model integrating XGBoost, Random Forest, and Antlion Optimization was deployed in India to predict infectious disease outbreaks. With over 21,000 samples, the model achieved over 96% accuracy, but the results were constrained by regional data limitations [@sivakumar2023prediction].

Applications in the financial sector also highlight XGBoost’s utility. Li et al. [@li2020xgboost] demonstrated its superiority over logistic regression in predicting loan defaults using the Lending Club dataset. The model’s ability to rank feature importance and prevent overfitting using regularization made it a top performer in credit risk prediction tasks. Similarly, Fomunyam [@fomunyam2023impact] applied XGBoost to forecast volatility in the U.S. stock market, identifying the Economic Policy Uncertainty Index as a critical predictor. While the model achieved modest accuracy (55.62%) and a low MCC (0.2793), it proved useful in highlighting economic indicators driving market fear.

In pharmaceutical research, Wiens et al. [@wiens2025drugdev] presented a comprehensive tutorial and use case for applying XGBoost in drug development. The study used the algorithm to predict disease risk progression, showcasing its effectiveness in clinical decision making contexts. The model's capacity to handle missing data, rank feature importance, and outperform traditional models in clinical trial data made it a valuable tool in the biomedical pipeline.

Further applications of XGBoost include educational diagnostics and sports analytics. For instance, Su et al. [@su2023knowledge] and Hakkal and Lahcen [@hakkal2024education] showed how XGBoost enhanced learner performance prediction and knowledge tracing on platforms like ASSIST09 and Algebra08. Nikolaidis et al. [@nikolaidis2023ultramarathon] used XGBoost to predict ultramarathon running speeds based on demographics and environmental factors, finding that country of origin and road surface type were strong predictors. In construction, Ren et al. [@ren2023strength] utilized XGBoost to predict the compressive strength of ultra-high-performance concrete, attaining 95.6% accuracy. However, initial overfitting required hyperparameter optimization, and the lack of data normalization was cited as a limitation.

XGBoost has also been instrumental in mental health classification studies using small or imbalanced datasets. Zhu, Shen, and Zhang [@zhang2022imbalanced] applied dual XGBoost models to distinguish between deficit and non-deficit schizophrenia subtypes using fMRI features, reaching an average accuracy of 73.89%. Saleh et al. [@saleh2024child] compared XGBoost with linear regression in predicting depression among refugee children, finding that XGBoost offered a richer, non-linear understanding of contributing factors such as sleep quality and stress. Lastly, a recent study applied XGBoost to classify mental health disorders, including Bipolar I, Bipolar II, Major Depressive Disorder, and Normal, using categorical symptom data. The model’s ability to rank symptom importance and identify complex patterns made it ideal for psychiatric diagnostics using structured survey inputs.

Altogether, these findings reinforce XGBoost’s versatility and robustness in handling high dimensional, imbalanced, and heterogeneous data. While performance can be compromised without proper preprocessing or tuning, the algorithm consistently outperforms traditional statistical models when paired with sampling strategies, optimization algorithms, and domain specific enhancements. Its strength lies in its balance of computational efficiency, predictive power, and interpretability, making it a benchmark tool in the expanding landscape of applied machine learning.



## Methods

### eXtreme Gradient Boosting (XGBoost) Model

The eXtreme Gradient Boosting (XGBoost) algorithm is a powerful ensemble learning method based on gradient-boosted decision trees. It builds an additive model in a forward stage-wise manner, where each new tree is trained to predict the residual errors of the ensemble built thus far. This iterative process allows the model to capture complex, non-linear relationships and interactions between features—in this case, across the 17 variables in the dataset.

XGBoost optimizes a regularized objective function that balances model accuracy with complexity, thereby reducing the risk of overfitting. The general form of the objective function is:

$$
\text{Obj} = \sum_{i=1}^{n} L(y_i, \hat{y}_i) + \sum_{k=1}^{K} \Omega(f_k)
$$
[@zhang2022imbalanced]

Where:

- The **loss function** measures the difference between the predicted and actual values:

$$
L(y_i, \hat{y}_i) = (y_i - \hat{y}_i)^2
$$

- The **regularization term** penalizes the complexity of each tree:

$$
\Omega(f_k) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^{T} w_j^2
$$

Here:

$$
T = \text{number of leaves in the tree}
$$

$$
w_j = \text{score (weight) on leaf } j
$$

$$
\gamma = \text{penalty for adding a new leaf}
$$

$$
\lambda = \text{L2 regularization term on leaf weights}
$$

Gradient descent is used to minimize this objective function by updating model parameters iteratively. The regularization components play a key role in controlling tree complexity, helping prevent overfitting, especially in high-dimensional or noisy datasets.

Hyperparameters such as the learning rate, maximum tree depth, regularization strength, and subsampling ratio are typically tuned via cross-validation to maximize model performance. Due to its efficiency and predictive power, XGBoost is widely used in structured data problems [@zhang2022imbalanced].

![XG Boost](XG Boost.png)
**Figure 1** shows the sequential 3-stage process of XGBoost:

Stage 1: Initial Prediction
The process begins by making a simple first guess, or "base prediction" . This initial model is usually very simple (like the average of all the data). The graph shows this simple model as a flat line that doesn't fit the data points well. The model then calculates the errors, or residuals, which are the differences between this first guess and the actual data points.

Stage 2: Iterative Boosting (Tree k)
This is the core loop of the algorithm. A new tree is built specifically to predict the errors from the previous stage. The goal of this new tree is to correct the mistakes its predecessor made. It does this by optimizing the objective function, which cleverly balances minimizing the new errors with a penalty for being too complex . This keeps the trees simple and prevents overfitting. The graph shows the loss being reduced as the model learns.

Stage 3: Final Ensemble Prediction
The final prediction is not just the last tree. Instead, it is the sum of all the individual trees added together (the initial guess + the first correction tree + the second correction tree , and so on). By combining all these simple models, each one correcting the last, the "Final Model" becomes a single, highly accurate predictor that fits the complex patterns in the data much better than any single tree could.

So we can summarize that the XGBoost model starts with a simple guess, then builds a series of new models to fix the errors of the previous ones, and finally, adds all these models up to get a single, strong prediction [@zhang2022imbalanced].


## Analysis and Results

```{r}
# Load libraries
library(knitr)
library(tidyverse)
library(xgboost)
library(ggplot2)
library(DataExplorer)
library(dplyr)
library(tidyr)
library(kableExtra)
library(vcd)
library(reshape2)
library(FSelectorRcpp)
library(magrittr) 
library(tidymodels)
library(iml)
library(vip)
library(shapviz)
library(recipes)
library(purrr)
library(rsample)
library(yardstick)
library(gt)
```

### Data Exploration and Visualization

The dataset used for this analysis was sourced from Kaggle (https://www.kaggle.com/datasets/cid007/mental-disorder-classification) and originates from a private psychologist's office, capturing patient data collected throughout 2023. It includes information on 120 individuals who were assessed for mental health conditions. Each patient was diagnosed with one of four possible mental health statuses: normal, bipolar type I, bipolar type II, or depression. The dataset contains 17 variables that were used to inform the diagnostic process, potentially covering a range of clinical, behavioral, and demographic factors. These variables form the basis for classification and analysis of the mental health conditions represented. **Table 1** provides a detailed overview of each variable included in the dataset, offering insight into the features considered during the diagnostic evaluation.

```{r}
# Create Table 1: Variable Names and Descriptions

# Define variables and descriptions
variables <- c(
  "Patient Number",
  "Sadness",
  "Euphoric",
  "Exhausted",
  "Sleep Disorder",
  "Mood Swing",
  "Suicidal Thoughts",
  "Anorexia",
  "Authority Respect",
  "Try-Explanation",
  "Aggressive Response",
  "Ignore & Move-On",
  "Nervous Breakdown",
  "Admit Mistakes",
  "Overthinking",
  "Sexual Activity",
  "Concentration",
  "Optimism",
  "Expert Diagnose"
)

descriptions <- c(
  "Unique identifier for each patient.",
  "How often the patient experiences sadness determined by 4 groupings (Seldom, Sometimes, Usually, Most-Often).",
  "How often the patient experiences a euphoric mood determined by 4 groupings (Seldom, Sometimes, Usually, Most-Often).",
  "How often the patient experiences feelings of physical or mental fatigue determined by 4 groupings (Seldom, Sometimes, Usually, Most-Often).",
  "How often the patient experiences presence and severity of sleep-related disturbances determined by 4 groupings (Seldom, Sometimes, Usually, Most-Often).",
  "If the patient emotional mood swings (Yes/No) response.",
  "If the patient has suicidal ideation (Yes/No) response.",
  "If the patient has indicators of disordered eating or significant weight loss (Yes/No) response.",
  "If the patient respects authority and rules (Yes/No) response.",
  "If the patient tries to explain their behavior or symptoms (Yes/No) response.",
  "If the patient has a tendency to respond to situations with aggression (Yes/No) response.",
  "If the patient has a tendency to dismiss issues and move forward without resolution (Yes/No) response.",
  "If the patient has incidences of emotional or psychological breakdowns (Yes/No) response.",
  "If the patient has a willingness to admit personal faults or mistakes (Yes/No) response.",
  "If the patient has a tendency to ruminate or overanalyze situations (Yes/No) response.",
  "Level of sexual interest or activity, ranked from 1 to 10.",
  "Ability to focus and maintain attention, ranked from 1 to 10.",
  "General outlook on life and future events ranked from 1 to 10.",
  "Final clinical diagnosis assigned by the expert (Normal, Bipolar I, Bipolar II, Depression)."
)

# Create data frame
table1 <- data.frame(
  Variable = variables,
  Description = descriptions,
  stringsAsFactors = FALSE
)

# Print Table 1

kable(table1, format = "html", table.attr = "style='width:100%;'")  # for HTML output


```

##### Initial Data Exploration

```{r}
#| include: false
data <- read.csv("Dataset-Mental-Disorders.csv", stringsAsFactors = FALSE)
# Drop Patient Number column 
data <- data %>% select(-`Patient.Number`)
```


```{r}

# Define variables to combine
included_vars <- c("Sexual.Activity", "Concentration", "Optimism")

# Pivot data to longer format for these variables
long_data <- data %>%
  select(all_of(included_vars)) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Response")

# Plot grouped bar chart
ggplot(long_data, aes(x = Response, fill = Variable)) +
  geom_bar(position = "dodge") +  # dodge = side-by-side bars
  labs(
    title = "Figure 2 Response Distribution for Sexual Activity, Concentration, and Optimism",
    x = "Response",
    y = "Count",
    fill = "Variable"
  ) +
  theme_minimal(base_size = 8) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

```

In **Figure** 2, the data appears to be normally distributed across the three variables, concetration, opitimism, and sexual activity which were all rated from 1 to 10 from the 120 patients.

```{r}
#| echo: false
# List of variables to exclude
excluded_vars <- c("Sexual.Activity", "Concentration", "Optimism")

# Create distribution table, excluding the listed variables
distribution_table <- data %>%
  select(-all_of(excluded_vars)) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value") %>%
  group_by(Variable, Value) %>%
  summarise(Count = n(), .groups = "drop") %>%
  arrange(Variable, desc(Count)) %>%
  group_by(Variable) %>%
  summarise(Distribution = paste(Value, Count, sep = ": ", collapse = ", "), .groups = "drop")

# Format 
distribution_table %>%
  kbl(
    caption = "Table 2 Category Distributions per Categorical Variable",
    align = c("l", "l"),
    col.names = c("Variable", "Category Distribution"),
    escape = FALSE  # Corrected from earlier error
  ) %>%
  kable_styling(
    full_width = FALSE,
    bootstrap_options = c("striped", "hover", "condensed", "responsive"),
    position = "center"
  ) %>%
  column_spec(2, width = "40em")
```

**Table 2** presents the distribution of the variable. The yes/no variables are approximately balanced across patient responses, indicating a well-distributed set of features. The target variable, Expert Diagnosis, is similarly balanced, with nearly 30 patients per diagnostic category. This distribution ensures that no single diagnosis dominates the dataset, making it suitable for robust analysis.


```{r}
#| include: false
# Separate target variable
target <- data$Expert.Diagnose

# Drop target from the features dataset
features <- data %>% select(-"Expert.Diagnose")

# Compute Cramer's V for each feature against the target
cramers_v <- sapply(names(features), function(col) {
  tbl <- table(features[[col]], target)
  assocstats(tbl)$cramer
})

# Display results
cramers_v <- sort(cramers_v, decreasing = TRUE)
cramers_v
```

```{r}
# Convert to data frame for plotting

cramers_df <- data.frame(
  Variable = names(cramers_v),
  CramersV = cramers_v
)

# Drop NA values
cramers_df <- cramers_df[!is.na(cramers_df$CramersV), ]

# Plot
ggplot(cramers_df, aes(x = reorder(Variable, CramersV), y = CramersV)) +
  geom_col(fill = "#2c7fb8") +
  coord_flip() +
  labs(title = "Figure 3 Cramer's V Association with Mental Health Diagnosis",
       x = "Feature",
       y = "Cramer's V") +
  theme_minimal()

```
**Figure 3** shows the association between categorical variables and the mental health diagnosis using Cramer's V. The results indicate that *Mood Swings* and *Suicidal Thoughts* exhibit the strongest correlation with the diagnosis.  

Cramer's V is calculated as:

$$
V = \sqrt{\frac{\chi^2 / n}{\min(k - 1, r - 1)}}
$$

where \(\chi^2\) is the chi-squared statistic for the contingency table, \(n\) is the total number of observations, \(k\) is the number of columns, and \(r\) is the number of rows in the table [@wucramersV].
 

##### Data Transformation & Cleaning 
To facilitate analysis, all categorical variables in the dataset were transformed into numerical or grouped categorical formats in Excel. The transformations were applied as follows:

* **Ordinal Frequency Responses** (e.g., *Sadness*, *Euphoric*, *Exhausted*):

  * *Seldom* → 1
  * *Sometimes* → 2
  * *Usually* → 3
  * *Most-Often* → 4

* **Binary Responses** (e.g., *Suicidal Thoughts*, *Mood Swing*):

  * *Yes* → 1
  * *No* → 0

* **Scaled Ratings (1–10)** (e.g., *Sexual Activity*, *Optimism*):
  These were grouped into three categorical levels to simplify analysis:

  * Ratings **1–3** → *Category 1*
  * Ratings **4–6** → *Category 2*
  * Ratings **7–9** → *Category 3*

These transformations standardized the dataset, allowing for easier correlation analysis, visualizations, and model input compatibility.

```{r}
#| include: false
datat <- read.csv("Transformed Dataset.csv", stringsAsFactors = FALSE)
```

```{r}
#| eval: false
#| include: false

# Exclude categorical variables
numeric_data <- datat[, !(names(datat) %in% c("Sexual.Activity", "Optimism", "Concentration"))]

# Ensure remaining variables are numeric
numeric_data <- numeric_data[, sapply(numeric_data, is.numeric)]

# Compute correlation matrix
cor_matrix <- cor(numeric_data, use = "complete.obs")

# Melt the matrix for plotting
melted_cor <- melt(cor_matrix)

# Create heatmap
ggplot(data = melted_cor, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "red", mid = "yellow", high = "green", 
                       midpoint = 0, limit = c(-1, 1), name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  coord_fixed() +
  labs(title = "Correlation Matrix Heatmap", x = "", y = "")

```


```{r}
#| include: false
# Compute Information Gain
info_gain <- information_gain(Expert.Diagnose ~ ., data = datat)

# View result in table form (sorted by importance)
info_gain_sorted <- info_gain[order(-info_gain$importance), ]
print(info_gain_sorted)

```

```{r}
# Plot using ggplot2
ggplot(info_gain_sorted, aes(x = reorder(attributes, importance), y = importance)) +
  geom_bar(stat = "identity", fill = "forestgreen") +
  coord_flip() +
  labs(title = "Figure 4 Information Gain by Variable",
       x = "Variable",
       y = "Information Gain") +
  theme_minimal()

```

**Figure 4** illustrates the results of an information gain analysis performed on the transformed dataset. Among the variables, *Mood Swings* (0.5759) and *Optimism* (0.2303) exhibit the highest information gain, indicating that they contribute most significantly to reducing uncertainty in predicting the target variable.  

Information gain is calculated as:

$$
IG(T, X) = H(T) - \sum_{v \in \text{Values}(X)} \frac{|T_v|}{|T|} H(T_v)
$$

where \(H(T)\) is the entropy of the target variable \(T\), \(X\) is the feature of interest, \(T_v\) is the subset of \(T\) where \(X\) takes value \(v\), and \(|\cdot|\) denotes the number of observations [@leeInfo].


### Modeling and Results

##### Overview of XG Boost Modeling Framework

The objective of this analysis is to develop a predictive model for mental health classification using a gradient boosting framework. Extreme Gradient Boosting (XGBoost) was selected for its computational efficiency, strong performance on small structured datasets, and ability to provide interpretable feature insights.

The dataset comprises 120 observations and 17 features, making XGBoost an appropriate choice given its efficiency with limited data. Its tree-based ensemble approach effectively captures nonlinear relationships and complex patterns without requiring the large sample sizes typically needed for deep learning models.

Moreover, XGBoost’s feature importance outputs enhance interpretability, an essential consideration in mental health research, where understanding variable contributions is as valuable as predictive accuracy. Finally, the algorithm’s optimized gradient boosting process allows for rapid experimentation with hyperparameters and validation folds, facilitating efficient model tuning and evaluation.

##### Hyperparameter Optimization

```{r}
#Preparing Transformed Dataset
datat <- datat %>%
  mutate(across(where(is.character), as.factor))
```

```{r}
# Make sure target factor levels are consistent
datat$Expert.Diagnose <- factor(datat$Expert.Diagnose)
```


```{r}
# Split into training and testing
set.seed(123)
data_split <- initial_split(datat, prop = 0.8, strata = Expert.Diagnose)
train_data <- training(data_split)
test_data  <- testing(data_split)
```

The dataset was randomly partitioned into an 80/20 training–testing split, resulting in 96 patient surveys used for model training and 24 surveys reserved for independent testing. The predictive target for all models was the expert clinical diagnosis, which served as the ground-truth reference label.

```{r}
# Begin XG Boost
xgb_recipe <- recipe(Expert.Diagnose ~ ., data = train_data) %>%
  step_dummy(all_nominal_predictors()) %>%  # Convert factors to dummies
  step_zv(all_predictors())                 # Remove zero-variance predictors

```

```{r}
# Define XGBoost model with tunable parameters
xgb_spec <- boost_tree(
  trees = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  min_n = tune(),
  sample_size = tune(),
  mtry = tune()
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

```

An XGBoost model specification was created using the tidymodels framework with all major hyperparameters set to be tuned, including the number of trees, tree depth, learning rate, loss reduction, minimum node size, subsampling rate, and number of predictors considered at each split. The model was configured to use the “xgboost” engine and trained in classification mode. This specification functions as the template for subsequent hyperparameter optimization within the workflow.

```{r}
#Creating Workflow
xgb_workflow <- workflow() %>%
  add_model(xgb_spec) %>%
  add_recipe(xgb_recipe)
```

```{r}
# Resampling method
set.seed(123)
folds <- vfold_cv(train_data, v = 5, strata = Expert.Diagnose)
```

Five-fold stratified cross-validation was used for hyperparameter tuning. The training dataset was partitioned into five equally sized folds with the class distribution of the outcome variable (Expert Diagnosis) preserved in each fold. For each tuning iteration, models were trained on four folds and validated on the remaining fold, ensuring stable and unbiased performance estimates. A fixed random seed was used to ensure reproducibility of the fold assignments.

```{r}
# Hyperparameter grid
xgb_grid <- grid_space_filling(
  trees(range = c(100, 1000)),
  tree_depth(range = c(2, 10)),
  learn_rate(range = c(0.001, 0.3)),
  loss_reduction(),
  min_n(),
  sample_size = sample_prop(),
  finalize(mtry(), train_data),
  size = 30
)
```

The hyperparameter grid for the XGBoost model was constructed using a **space-filling design** to efficiently explore the multi-dimensional parameter space. This approach ensures broad coverage of plausible hyperparameter combinations while avoiding the combinatorial explosion of a full grid search. Specifically, the grid includes the following parameters:

- **Number of Trees (`trees`)**:  
  Set between 100 and 1000 to capture both shallow ensembles and deeper ensembles capable of modeling complex relationships. This range balances expressiveness with computational efficiency.

- **Tree Depth (`tree_depth`)**:  
  Ranges from 2 to 10, allowing the exploration of simple trees that generalize well as well as deeper trees that can capture intricate feature interactions.

- **Learning Rate (`learn_rate`)**:  
  Varies from 0.001 to 0.3. Lower values provide finer control over updates, potentially improving generalization, while higher values allow faster convergence at the risk of overshooting the optimal solution.

- **Loss Reduction (`loss_reduction`)**:  
  Controls the minimum reduction in loss required to create a split, helping to prevent unnecessary splits and reduce overfitting.

- **Minimum Node Size (`min_n`)**:  
  Specifies the minimum number of observations per terminal node, balancing the ability to capture fine-grained patterns with regularization.

- **Subsampling (`sample_size`)**:  
  Represented as a proportion of the training data, introducing stochasticity to improve generalization.

- **Number of Predictors Considered per Split (`mtry`)**:  
  Finalized based on the dimensions of the training dataset to ensure meaningful splits.

The final grid consists of **30 candidate hyperparameter combinations**, which provides a compromise between computational feasibility and adequate coverage of the parameter space.

Using a space-filling design (`grid_space_filling`) offers several advantages:

1. **Efficiency:** Broadly explores the parameter space with fewer evaluations.  
2. **Coverage:** Reduces the risk of missing promising hyperparameter configurations.  
3. **Scalability:** Remains computationally feasible for large datasets or models with many parameters.

In summary, this hyperparameter grid balances model flexibility, generalization, and computational efficiency, allowing XGBoost to explore a wide range of model complexities while incorporating key regularization strategies to reduce overfitting [@Geron2019].


```{r}
# Tuning the model
set.seed(123)
xgb_tune_results <- tune_grid(
  xgb_workflow,
  resamples = folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE),
  metrics = metric_set(accuracy, roc_auc)
)

```

Hyperparameter tuning was performed using 5-fold stratified cross-validation with a space-filling grid of 30 candidate parameter combinations. For each combination, models were trained on four folds and evaluated on the fifth, cycling through all folds. Model performance was assessed using accuracy and the area under the ROC curve (ROC AUC). Predictions from each resample were saved to support additional diagnostic analyses. A fixed random seed ensured reproducibility of all results.

```{r}
# Visualize tuning results
autoplot(xgb_tune_results)
```
```{r}
tune_df <- collect_metrics(xgb_tune_results)

# Plot ROC AUC vs number of trees
ggplot(tune_df, aes(x = trees, y = mean, color = factor(tree_depth))) +
  geom_point() +
  geom_line() +
  facet_wrap(~ .metric, scales = "free_y") +
  labs(
    x = "Number of Trees",
    y = "Mean Performance",
    color = "Tree Depth"
  ) +
  theme_minimal()

```
Visualizing the tuning results revealed clear performance patterns across the explored hyperparameter space. The ROC AUC versus number of trees plot indicated that the optimal tree depth was approximately 2, suggesting that relatively shallow trees generalized best for this dataset. Model performance increased steadily with the number of boosting iterations, with the highest ROC AUC occurring at around 625 trees. At this configuration, the model achieved an accuracy exceeding 0.80 and a ROC AUC greater than 0.90, indicating strong discriminatory performance.

```{r}
# Selecting best parameters
best_params <- select_best(xgb_tune_results, metric = "roc_auc")

```

```{r}

best_params_table <- best_params %>%
  as_tibble() %>%                # ensure it's a tibble
  mutate(across(where(is.numeric), ~ round(.x, 4)))  # round numeric values


best_params_table %>%
  gt() %>%
  tab_header(
    title = "Best Hyperparameter Combination",
    subtitle = "Selected based on ROC AUC"
  ) %>%
  fmt_number(
    columns = where(is.numeric),
    decimals = 4
  ) %>%
  cols_label(
    trees = "Number of Trees",
    tree_depth = "Tree Depth",
    learn_rate = "Learning Rate",
    loss_reduction = "Loss Reduction",
    min_n = "Min Node Size",
    sample_size = "Subsample Size",
    mtry = "Number of Predictors"
  )

```

Although visual inspection of the tuning plots provided a general sense of the model’s performance trends, identifying the exact optimal hyperparameters was not straightforward due to interaction effects between parameters and the non-linear nature of boosted tree models. Therefore, the select_best() function from the tidymodels framework was used to systematically extract the hyperparameter combination that achieved the highest ROC AUC across all resampled evaluations.

The resulting optimal configuration included 3 predictors (mtry = 3), 627 boosting iterations, a minimum node size (min_n) of 2, a learning rate of 0.16501, a loss-reduction parameter of 0.0034, and a subsample proportion of 0.5655. This combination represents the tuning grid entry labeled pre0_mod06_post0, which achieved the best balance between model complexity and predictive performance during cross-validation.

This automated selection approach ensures that the final model parameters are chosen objectively based on empirical performance rather than subjective visual interpretation, which can be challenging when multiple hyperparameters interact.

```{r}
# Finalize the model with best params
final_xgb <- finalize_workflow(xgb_workflow, best_params)
```

```{r}
# Fit the finalized workflow on the full training data
final_fit <- fit(final_xgb, data = train_data)

# Compute and plot feature importance
vip(final_fit$fit$fit, geom = "col")

```
Using the best parameter fit the has the top five features in order of importance as mood swings, sadness, agressive response, exhausted, and euphoric.

```{r}
#Prediction Visualization
preds2 <- predict(final_fit, new_data = test_data, type = "class") %>%
  bind_cols(test_data %>% select(Expert.Diagnose)) %>%
  rename(.pred_class = .pred_class)

# Plot
ggplot(preds2, aes(x = factor(.pred_class), fill = factor(Expert.Diagnose))) +
  geom_bar(position = "dodge") +
  labs(x = "Predicted Class", fill = "True Class") +
  theme_minimal()

```
The model's predictions on the held-out test set showed strong agreement with the expert diagnoses. Specifically:

- **Bipolar Type-1:** 4 of 6 cases were correctly predicted (66.7%)  
- **Bipolar Type-2:** 5 of 7 cases were correctly predicted (71.4%)  
- **Depression:** 6 of 7 cases were correctly predicted (85.7%)  
- **Normal:** 4 of 6 cases were correctly predicted (66.7%)  

Overall, these results indicate that the model accurately identified the majority of cases in each diagnostic category, demonstrating good discriminatory performance across the four classes.


##### Model Evaluation & Performance 

```{r}
# Fitting final model on all training data
final_fit <- fit(final_xgb, data = train_data)
```

```{r}
# Evaluate the final model on test data
test_results <- predict(final_fit, test_data) %>%
  bind_cols(predict(final_fit, test_data, type = "prob")) %>%
  bind_cols(test_data %>% select(Expert.Diagnose))

```

```{r}
# Confusion Matrix
conf_mat(test_results, truth = Expert.Diagnose, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```


```{r}
# Filter correct probability columns
true_levels <- levels(train_data$Expert.Diagnose)
prob_cols <- paste0(".pred_", true_levels)

# Compute model performance metrics
metrics(test_results, truth = Expert.Diagnose, estimate = .pred_class)

roc_auc(test_results, truth = Expert.Diagnose,
        all_of(prob_cols),
        estimator = "macro_weighted")


```




```{r}
# ROC Curve (macro average)
roc_curve(test_results, truth = Expert.Diagnose, all_of(prob_cols)) %>%
  autoplot()

```

##### Feature Importance & Interepretability

```{r}

# Extract fitted workflow and XGBoost model

wf_fit <- final_fit
xgb_fit <- wf_fit %>%
extract_fit_engine()

# Extract recipe and bake training data into numeric matrix

rec <- wf_fit %>% extract_recipe()

train_baked <- bake(rec, new_data = train_data)

# Separate predictors and target

X_train <- train_baked %>% select(-Expert.Diagnose)
X_train_matrix <- as.matrix(X_train)

```

```{r}
# Compute SHAP values

sv <- shapviz(
xgb_fit,
X_train_matrix,
X = X_train
)

```

```{r}
# Global feature importance

sv_importance <- sv_importance(sv)
sv_importance

```


```{r}
# Pick an observation

i <- 1

sv_waterfall(sv, row_index = i)

```



##### Sensitvity & Robustness Analysis

```{r}
# Repeated stratified k-fold CV
set.seed(123)
repeated_folds <- vfold_cv(
  train_data,
  v = 5,
  repeats = 5,
  strata = Expert.Diagnose
)

```


```{r}
#K Fold Cross Validation for Robustness

cv_results <- map_dfr(repeated_folds$splits, function(split) {
  
  # Fit workflow
  fit_wf <- fit(final_xgb, data = analysis(split))
  
  # Predict class
  pred_class <- predict(fit_wf, new_data = assessment(split)) %>%
    rename(.pred_class = .pred_class)
  
  # Predict probabilities
  pred_prob <- predict(fit_wf, new_data = assessment(split), type = "prob")
  
  # Truth
  truth <- assessment(split) %>% select(Expert.Diagnose)
  
  # Combine predictions
  preds <- bind_cols(pred_class, pred_prob, truth)
  
  # Only probability columns (exclude .pred_class)
  prob_cols <- colnames(preds)[grepl("^\\.pred_", colnames(preds)) & colnames(preds) != ".pred_class"]
  
  tibble(
    accuracy = accuracy(preds, truth = Expert.Diagnose, estimate = .pred_class)$.estimate,
    roc_auc  = roc_auc(preds, truth = Expert.Diagnose, !!!syms(prob_cols), estimator = "macro_weighted")$.estimate
  )
})

cv_results %>%
  summarise(
    mean_accuracy = mean(accuracy),
    sd_accuracy   = sd(accuracy),
    mean_roc      = mean(roc_auc),
    sd_roc        = sd(roc_auc)
  )


```


##### Summary of Modeling Results

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References


---
title: "Using XGBoost in Mental Disorder Classification"
subtitle: "Fall 2025"
author: "Aabiya Mansoor, Abigail Penza Jackson, Corina Rich, Madelyn Champion (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    theme: lux
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

::: callout-important
**Remember:** Your goal is to make your audience understand and care
about your findings. By crafting a compelling story, you can effectively
communicate the value of your data science project.

Carefully read this template since it has instructions and tips to
writing!

Nice report!
:::

## Introduction

eXtreme Gradient Boosting (XGBoost) is a state-of-the-art, supervised machine learning algorithm renowned for its high performance, speed, and scalability [1]. Developed as an optimized implementation of the gradient boosting framework, XGBoost builds a powerful predictive model by sequentially creating an ensemble of weak decision trees, where each new tree is trained to correct the residual errors of the previous ones [1]. Its importance in the data science community stems from several key architectural advantages, including a sparsity-aware algorithm for handling missing or zero-value data, parallel processing capabilities for faster computation, and built-in L1 and L2 regularization to prevent model overfitting. The algorithm's versatility and effectiveness have been demonstrated across a wide range of domains, from achieving high accuracy in financial credit scoring [2] to enhancing learner performance prediction in education [3].
This research applies the power of XGBoost to address the significant and growing challenge of mental disorder classification. The early and accurate identification of mental health conditions is crucial for effective intervention, yet traditional diagnostic methods can be subjective, resource-intensive, and inaccessible to many. Structured clinical and survey data provide an opportunity to apply computational methods to find objective patterns that may aid in this diagnostic process. The objective of this study is to develop and evaluate a robust XGBoost model for the multi-class classification of individuals into one of four categories: Bipolar I Disorder, Bipolar II Disorder, Major Depressive Disorder, and Normal [6].
The selection of XGBoost for this complex classification problem, containing information of 120 patients associated with 17 symptoms, is particularly strategic. The dataset consists of tabular survey responses, where the relationships between different answers (features) and the final diagnosis are complex and non-linear. XGBoost excels at identifying these intricate patterns in structured data. Its ability to perform feature importance ranking will also help in understanding which survey questions are most predictive of a specific mental health condition. Furthermore, XGBoost's built-in regularization is crucial for preventing the model from overfitting to the training data, ensuring it can generalize accurately to new, unseen patient cases. These features make XGBoost an exceptionally suitable framework for the nuanced task of classifying mental health disorders based on categorical survey data.

## Literature Review

Recent advancements in machine learning, particularly the Extreme Gradient Boosting (XGBoost) algorithm, have shown promising applications across a variety of disciplines, including engineering, healthcare, sports science, and economics. In the construction industry, XGBoost was applied to predict the 28-day compressive strength of ultra-high-performance concrete (UHPC) incorporating low-carbon and recycled materials, achieving an accuracy of 95.6% and maintaining error margins within Â±15% [1]. The model effectively handled outliers and complex variables, though initial overfitting required hyperparameter optimization. The absence of data normalization was also noted as a limitation. In sports science, XGBoost was used to predict running speed in 10-day ultramarathon events based on athlete demographics and environmental factors [2]. The model identified country of origin and age group as the most influential predictors, with asphalt surfaces supporting faster speeds. However, the geographic concentration of the dataset limited generalizability.
In the medical domain, XGBoost has proven useful in identifying clinical risk factors. One study used a hybrid ensemble model to predict suicide attempts among first-episode major depressive disorder (MDD) patients, finding that psychotic symptoms (e.g., excitement, hostility, anxiety) and elevated thyroid function were strong predictors of risk within 14 days [3]. While the model performed well retrospectively, it lacked prospective predictive power and omitted key psychosocial variables. Another study compared XGBoost with other models such as CatBoost and LightGBM for diagnosing MDD using EEG and demographic data [4]. XGBoost achieved 92.5% accuracy, 76% precision, and 94.2% recall, closely trailing CatBoost. The authors emphasized the value of incorporating diverse datasets for more personalized mental health assessments.
In public health, a hybrid model combining XGBoost and Random Forest, with Antlion Optimization (ALO) for feature selection, was used to predict seasonal infectious disease outbreaks in India [5]. Based on over 21,000 samples, the model achieved 96.17% accuracy, 93.95% precision, and 95.86% recall, although its applicability was limited to the regional context. In finance, XGBoost was applied to assess the impact of U.S. macroeconomic variables on the CBOE Volatility Index (VIX), widely viewed as a market fear indicator [6]. While predictive accuracy was modest (55.62%) with a Matthews Correlation Coefficient (MCC) of 0.2793, the model identified the Economic Policy Uncertainty Index (Infectious Disease Tracker) as the most influential feature. Together, these studies demonstrate XGBoost's versatility in modeling complex, high-dimensional problems, though recurring challenges such as overfitting, limited sample diversity, and the need for prospective validation persist across domains.


## Methods

-   Detail the models or algorithms used.

-   Justify your choices based on the problem and data.

*The common non-parametric regression model is*
$Y_i = m(X_i) + \varepsilon_i$*, where* $Y_i$ *can be defined as the sum
of the regression function value* $m(x)$ *for* $X_i$*. Here* $m(x)$ *is
unknown and* $\varepsilon_i$ *some errors. With the help of this
definition, we can create the estimation for local averaging i.e.*
$m(x)$ *can be estimated with the product of* $Y_i$ *average and* $X_i$
*is near to* $x$*. In other words, this means that we are discovering
the line through the data points with the help of surrounding data
points. The estimation formula is printed below [@R-base]:*

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$$W_n(x)$ *is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if* $X_i$ *is far from* $x$*.*

*Another equation:*

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data Exploration and Visualization

-   Describe your data sources and collection process.

-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{python}
import pandas as pd
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References
